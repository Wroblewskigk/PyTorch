# Essential Torch.Tensor Features for AI \& ML

**Data Types and Precision Controls**
PyTorch tensors support a wide range of dtypes—from 32-bit and 16-bit floats (including Brain Floating Point and 
binary16), through 8-bit and 4-bit quantized integers, to complex types—allowing AI \& ML practitioners to balance 
precision, dynamic range, and performance. Employ lower-precision types (float16, bfloat16, quantized dtypes) during 
training or inference to accelerate computation and reduce memory footprint, while using float32 or float64 where higher 
numerical fidelity is critical.

**Tensor Creation and Device Placement**
Factory functions (e.g., `torch.zeros`, `torch.ones`, `torch.tensor`) let you specify `dtype` and `device` (CPU or CUDA) 
at construction. Creating tensors directly on the GPU avoids costly host-to-device transfers. Use `tensor.to(device)` or 
`.cuda()` for explicit device transfers. Consistently placing model parameters and data on the same device minimizes 
synchronization overhead and maximizes throughput.

**Autograd and Gradient Tracking**
Setting `requires_grad=True` enables automatic differentiation, which records tensor operations in a computation graph. 
Calling `.backward()` computes gradients in place, stored in `.grad` attributes. Understanding in-place versus 
out-of-place ops (methods with an underscore suffix mutate tensors) is vital to avoid inadvertent graph corruption and 
memory leaks.

**Indexing, Views, and Memory Efficiency**
Standard Python indexing and slicing (`x[^1]`, `x[:,2]`) yield tensor views without copying data. Use `.item()` to 
extract scalars. Leverage view operations (e.g., `.T`, `.view()`, `.permute()`) for reshaping and dimension manipulation 
without additional memory allocation. This is crucial in large-scale models where minimizing copies can significantly 
reduce memory overhead.

# Critical CUDA Semantics for AI \& ML Performance

**Device Management and Data Transfers**

- PyTorch tracks a “current” CUDA device: tensors created with `.cuda()` or on a `torch.device('cuda')` go to this 
- device by default.
- Use context managers (`with torch.cuda.device(i):`) or explicit device arguments for multi-GPU setups.
- Cross-GPU ops require explicit copy calls or peer-to-peer access; unsynchronized access across GPUs will error.

**TensorFloat-32 (TF32) for Mixed Precision**
On Ampere-class GPUs and later, TF32 accelerates matrix multiplies and convolutions by using 10-bit mantissa rounding 
with FP32 accumulation. Enabling TF32 (via `torch.backends.cuda.matmul.allow_tf32` and 
`torch.backends.cudnn.allow_tf32`) can yield ~7× speedups on tensor cores with only ~0.22% relative error—ideal for AI 
workloads tolerant to minor precision loss.

**Reduced-Precision GEMMs (FP16/BF16)**

- **FP16 GEMMs** optionally perform internal reductions in FP16 for higher throughput; disable via `allow_fp16_reduced_precision_reduction` when precision is paramount.
- **BF16 GEMMs** behave similarly, enabled by default, and can be disabled for numerical stability.
- **Full FP16 accumulation** (`allow_fp16_accumulation=True`) further speeds up GEMMs on Volta+ GPUs, at the cost of overflow risk.

**Asynchronous Execution and Streams**
CUDA kernels launch asynchronously: operations are enqueued on a default stream and executed later, overlapping compute 
and transfer. For accurate timing, use CUDA events (`torch.cuda.Event`) and `torch.cuda.synchronize()`. Create 
additional streams (`torch.cuda.Stream`) to parallelize independent tasks, but ensure proper stream synchronization 
(`.wait_stream()`, `.record_stream()`) to avoid race conditions or memory hazards.

**Memory Management and Custom Allocators**
PyTorch’s caching allocator reuses freed GPU memory for speed, but unused cached blocks still appear in `nvidia-smi`. 
Monitor memory with `memory_allocated()`, `memory_reserved()`, and release unused cache via `empty_cache()`. 
Fine-tune allocator behavior using `PYTORCH_CUDA_ALLOC_CONF` options (e.g., `backend`, `max_split_size_mb`, 
`roundup_power2_divisions`, `garbage_collection_threshold`) to reduce fragmentation and handle dynamic batch sizes. 
Advanced users can integrate custom allocators (e.g., NCCL’s `ncclMemAlloc`) via `torch.cuda.MemPool` for optimized 
distributed training or memory placement.

**Device-Agnostic Code and Pinned Memory**
Adopt patterns that detect CUDA availability (`torch.cuda.is_available()`) and set `args.device` accordingly, then 
create tensors and modules on this `device`. Use `pin_memory=True` in data loaders and non-blocking transfers 
(`non_blocking=True`) for faster host-to-GPU copies, which is essential for high-throughput training pipelines.

**Distributed Training Best Practices**
Prefer `torch.nn.parallel.DistributedDataParallel` (DDP) over `DataParallel` for multi-GPU training. DDP’s multiprocess 
approach (one process per GPU) avoids Python GIL contention and scales efficiently across nodes. Ensure proper 
environment setup (e.g., `CUDA_VISIBLE_DEVICES`, NCCL settings) and use collective memory pools when leveraging NVLink 
for inter-GPU communication.

**CUDA Graphs for CPU-Overhead Reduction**
CUDA Graphs capture and replay static sequences of GPU operations, eliminating per-kernel dispatch overhead. Use 
`torch.cuda.CUDAGraph` or the higher-level `torch.cuda.graph` context manager for workloads with fixed control flow and 
tensor shapes. Manage long-lived input/output tensors for repeated replays, and isolate optimizer steps or AMP scaling 
to maintain correctness. When paired with DDP and AMP, follow PyTorch guidelines (disable async error handling, warmup 
iterations) to safely capture collective operations in the graph.

